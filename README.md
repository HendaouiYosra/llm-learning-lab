# üöÄ Learning LLMs with Ollama & Python  

This repository contains the codes I wrote while learning about **Large Language Models (LLMs)** using **Ollama**. The goal is to explore different ways to interact with LLMs, 
customize their behavior, and optimize their responses using Python.


## üìÇ **What‚Äôs Inside?**
This repository is structured into different sections, each focusing on a key aspect of working with LLMs:

1Ô∏è‚É£ **01-How to Run LLM Models**  
   - Learn how to run LLMs in different ways:
     - Command Line (`cmd`)
     - API Calls (`requests`)
     - Using the **Ollama Python Library**
     - Using Google Colab

2Ô∏è‚É£ **02-Ollama Python Library**  
   - Hands-on examples of using the **Ollama Python package** to interact with models efficiently in Python.

3Ô∏è‚É£ **03-The Power of Modelfiles**  
   - Understand how to **customize model behavior** using **Ollama Modelfiles**.
   - Modify system prompts, adjust parameters, and create task-specific models.

4Ô∏è‚É£ **04-Prompt Engineering**  
   - Experiment with **prompt engineering techniques** to guide model responses for better accuracy and relevance.

5Ô∏è‚É£ **05-RAG (Retrieval-Augmented Generation)**  
   - Explore **RAG techniques** to enhance LLMs with external knowledge for better and more up-to-date answers.

---

## üöÄ **Getting Started with Ollama**

### 1Ô∏è‚É£ **Install Ollama**
Download and install Ollama from the official website:  
üîó [Ollama Download](https://ollama.com)

### 2Ô∏è‚É£ **Pull a Model**  
Ollama runs local models. Pull a model using:  
```bash
ollama pull llama3

``` 
**Then Start Your LLM Journey!**
